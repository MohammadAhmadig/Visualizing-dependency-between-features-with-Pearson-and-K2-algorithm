{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manufacturer, product type\n",
      "product type, flash memory installed\n",
      "product type, battery\n",
      "product type, supported digital audio standards\n",
      "flash memory installed, diagonal size\n",
      "diagonal size, supported digital audio standards\n",
      "supported digital audio standards, estimated battery life\n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n",
      "-18161.1524461918\n",
      "14204.8660998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import collections\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#Outputs graph edges to .gph file\n",
    "def graph_out(dag,filename,mapping):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(np.size(dag[0])):\n",
    "            for j in range(np.size(dag[0])):\n",
    "                if(dag[i][j] == 1):\n",
    "                    out_string = mapping[i] + ', ' + mapping[j] + '\\n'\n",
    "                    f.write(out_string)\n",
    "\n",
    "#Creates a dictionary of node index to category strings\n",
    "def map_categories(categories):\n",
    "    mapping = {}\n",
    "    for i in range(len(categories)):\n",
    "        mapping[i] = categories[i][0:len(categories[i])] \n",
    "    return mapping\n",
    "\n",
    "#Given a target, find every instance index in array\n",
    "def find(arr,target):\n",
    "    array = np.array([],dtype='float')\n",
    "    for i in range(np.size(arr)):\n",
    "        if(arr[i] == target):\n",
    "            array = np.append(array,i)\n",
    "    return array\n",
    "\n",
    "#Ln gamma function ln((x-1)!) ->  ln(0) + ln(1) + ... + ln(x-1)\n",
    "def ln_gamma(x):\n",
    "    return sum(np.log(range(1,int(x))))\n",
    "\n",
    "#Construct a data structure that stores the possible states (col) for each variable (row)\n",
    "#Also returns a range vector that stores the number of states for each variable\n",
    "def get_dim_range(_data, vec):\n",
    "        count_n = 0\n",
    "        d = np.size(vec[0,:])\n",
    "        dim_length = np.zeros((1,d),dtype = 'float')\n",
    "        t = -1\n",
    "        #Count number of states\n",
    "        for q in range(d):\n",
    "            temp_vec = np.unique(_data[:,vec[:,q]])\n",
    "            x = temp_vec.reshape(1,np.size(temp_vec))\n",
    "            temp_vec = x\n",
    "            if(temp_vec[:,0] == -1):\n",
    "                temp_vec = np.empty()\n",
    "            range_n = np.size(temp_vec)\n",
    "            dim_length[0,q] = range_n\n",
    "            t += 1\n",
    "            #Assign zeros to the end to create valid matrix dimensions.\n",
    "            if(count_n == 0):\n",
    "                count_n = range_n\n",
    "                dim = np.zeros((d,count_n),dtype = 'float')\n",
    "                dim[t,:] = temp_vec\n",
    "            elif(count_n >= range_n):\n",
    "                dim[t,:] = np.concatenate((temp_vec,np.zeros((1,count_n - range_n))),axis=1)\n",
    "            elif(count_n < range_n):\n",
    "                dim = np.concatenate((dim,np.zeros((d,range_n - count_n))),axis=1)\n",
    "                #print(dim.shape)\n",
    "                #print(temp_vec.shape)\n",
    "                dim[t,:] = temp_vec\n",
    "                count_n = range_n\n",
    "        return dim,dim_length\n",
    "\n",
    "\n",
    "def score(blob,var,var_parents):\n",
    "    score = 0\n",
    "    n = blob.n_samples\n",
    "    dim_var = blob.var_range_length[0,var]\n",
    "    range_var = blob.var_range[var,:]\n",
    "    r_i = dim_var\n",
    "    data_o = blob.data\n",
    "    used = np.zeros(n,dtype='float')\n",
    "    d = 1\n",
    "    #Get first unproccessed sample\n",
    "    while(d <= n):\n",
    "        freq = np.zeros(int(dim_var),dtype='float')\n",
    "        while(d <= n and used[d-1] == 1):\n",
    "            d += 1;\n",
    "        if(d > n):\n",
    "            break\n",
    "        for i in range(int(dim_var)):\n",
    "            if(range_var[i] == data_o[d-1,var]):\n",
    "                break\n",
    "        freq[i] = 1\n",
    "        used[d-1] = 1\n",
    "        parent = data[d-1,var_parents]\n",
    "        d += 1\n",
    "        if(d > n):\n",
    "            break\n",
    "        #count frequencies of states while keeping track of used samples\n",
    "        for j in range(d-1,n):\n",
    "            if(used[j] == 0):\n",
    "                if((parent==data[j,var_parents]).all()):\n",
    "                    i = 0\n",
    "                    while range_var[i] != data[j,var]:\n",
    "                        i += 1\n",
    "                    freq[i] += 1\n",
    "                    used[j] = 1\n",
    "        sum_m = np.sum(freq)\n",
    "        r_i = int(r_i)\n",
    "        #Finally, sum over frequencies to get log likelihood bayesian score\n",
    "        #with uniform priors\n",
    "        for j in range(1,r_i+1):\n",
    "            if(freq[j-1] != 0):\n",
    "                score += ln_gamma(freq[j-1]+1)\n",
    "        score += ln_gamma(r_i) - ln_gamma(sum_m + r_i)\n",
    "    return score\n",
    "\n",
    "#Data structure to hold samples and dimension state info.\n",
    "class data_blob:\n",
    "    def __init__(self, _data):\n",
    "        self.var_number = np.size(_data[0,:])\n",
    "        self.n_samples = np.size(_data[:,0])\n",
    "        self.data = _data\n",
    "        (self.var_range, self.var_range_length) = get_dim_range(_data,np.arange(0,self.var_number).reshape(1,self.var_number))\n",
    "\n",
    "\n",
    "#k2 uses scoring function to iteratively find best dag given a topological ordering\n",
    "def k2(blob,order,constraint_u):\n",
    "    dim = blob.var_number\n",
    "    dag = np.zeros((dim,dim),dtype='float')\n",
    "    k2_score = np.zeros((1,dim),dtype='float')\n",
    "    for i in range(1,dim):\n",
    "        parent = np.zeros((dim,1))\n",
    "        ok = 1\n",
    "        p_old = -1e10\n",
    "        while(ok == 1 and np.sum(parent) <= constraint_u):\n",
    "            local_max = -10e10\n",
    "            local_node = 0\n",
    "            #iterate through possible parent connections to determine best action\n",
    "            for j in range(i-1,-1,-1):\n",
    "                if(parent[order[j]] == 0):\n",
    "                    parent[order[j]] = 1\n",
    "                    #score this node\n",
    "                    local_score = score(blob,order[i],parent[:,0]==1)\n",
    "                    #determine local max\n",
    "                    if(local_score > local_max):\n",
    "                        local_max = local_score\n",
    "                        local_node = order[j]\n",
    "                    #mark parent processed\n",
    "                    parent[order[j]] = 0\n",
    "            #assign the highest parent\n",
    "            p_new = local_max\n",
    "            if(p_new > p_old):\n",
    "                p_old = p_new\n",
    "                parent[local_node] = 1\n",
    "            else:\n",
    "                ok = 0\n",
    "        k2_score[0,order[i]] = p_old\n",
    "        dag[:,order[i]] = parent.reshape(blob.var_number)\n",
    "    return dag, k2_score\n",
    "\n",
    "# Load data\n",
    "data_set = 'train1.csv'\n",
    "categories = np.genfromtxt(data_set, delimiter=',', max_rows=1, dtype=str)\n",
    "data = genfromtxt(data_set, dtype='float', delimiter=',',skip_header=True)\n",
    "\n",
    "\n",
    "#initialize \"the blob\" and map its variable names to indicies\n",
    "g = data_blob(data)\n",
    "\n",
    "mapping = map_categories(categories)\n",
    "#set the maximum number of parents any node can have\n",
    "iters = 1\n",
    "p_lim_max = 5\n",
    "#iterate from p_lim_floor to p_lim_max with random restart\n",
    "p_lim_floor = 4\n",
    "best_score = -10e10\n",
    "best_dag = np.zeros((1,1))\n",
    "time.clock()\n",
    "for i in range(iters):\n",
    "    for u in range(p_lim_floor,p_lim_max):\n",
    "        #generate random ordering\n",
    "        order = np.arange(g.var_number) \n",
    "        (dag,k2_score) = k2(g,order,u)\n",
    "        score = np.sum(k2_score)\n",
    "        if(score > best_score):\n",
    "            best_score = score\n",
    "            best_dag = dag\n",
    "\n",
    "# save bayesian network between features in file and print\n",
    "filename = 'train1.gph'\n",
    "graph_out(dag,filename,mapping)\n",
    "ff=open(filename, 'r')\n",
    "print(ff.read())\n",
    "print(dag)\n",
    "print(score)\n",
    "print(time.clock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
